{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Xử lý data gốc về data bỏ đầu và cuối, ngoài ra giúp đỡ lỗi về decord"
      ],
      "metadata": {
        "id": "PUMRFRdX5SvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n"
      ],
      "metadata": {
        "id": "36wj--HK4gV5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d bluebirdss/baole-dataset"
      ],
      "metadata": {
        "id": "ZNjoWRrQ41XB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c00834-8ad5-40be-c206-d12129718058"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading baole-dataset.zip to /content\n",
            "100% 4.92G/4.93G [01:05<00:00, 123MB/s] \n",
            "100% 4.93G/4.93G [01:05<00:00, 80.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip baole-dataset.zip"
      ],
      "metadata": {
        "id": "82sLMkB-442F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code xử lý data part 1\n"
      ],
      "metadata": {
        "id": "Y9nNTZsD5hWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cvTFISXs5uCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def process_video(input_video_path, output_video_path):\n",
        "    # Khởi tạo đối tượng VideoCapture\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    # Lấy tổng số frames và FPS của video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    duration = total_frames / original_fps\n",
        "    # Tính toán phần trăm và thời điểm bắt đầu, kết thúc để cắt\n",
        "    if duration > 60:\n",
        "        start_percent = 15\n",
        "        end_percent = 85\n",
        "    elif duration > 10:\n",
        "        start_percent = 10\n",
        "        end_percent = 90\n",
        "    else:\n",
        "        start_percent = 0\n",
        "        end_percent = 100\n",
        "\n",
        "    start_frame = int((start_percent / 100) * total_frames)\n",
        "    end_frame = int((end_percent / 100) * total_frames)\n",
        "\n",
        "    # Lấy kích thước frame của video\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Tạo đối tượng VideoWriter với FPS là desired_fps\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, original_fps, (frame_width, frame_height))\n",
        "\n",
        "    # Đọc từng frame và ghi frame vào video mới nếu nằm trong khoảng đã tính và theo tỷ lệ FPS mới\n",
        "    current_frame = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Chỉ xử lý frame nếu nằm trong khoảng thời gian đã định\n",
        "        if start_frame <= current_frame <= end_frame:\n",
        "            out.write(frame)\n",
        "\n",
        "        current_frame += 1\n",
        "\n",
        "    # Giải phóng tài nguyên\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(\"Video đã được xử lý và lưu tại: \", output_video_path)"
      ],
      "metadata": {
        "id": "ZsW7b8zy5qUX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = \"/content/data_mae/test\"\n",
        "output_dir = \"/content/output2/test\"\n",
        "\n",
        "for className in os.listdir(data_dir):\n",
        "    class_dir = os.path.join(data_dir, className)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for video_file in os.listdir(class_dir):\n",
        "            input_video_path = os.path.join(class_dir, video_file)\n",
        "            filename_without_extension, _ = os.path.splitext(video_file)\n",
        "            output_video_path = f\"{output_dir}/{className}/{filename_without_extension}.mp4\"\n",
        "            print(filename_without_extension)\n",
        "            os.makedirs(f\"{output_dir}/{className}\", exist_ok=True)\n",
        "            process_video(input_video_path, output_video_path)\n"
      ],
      "metadata": {
        "id": "NAr6r5YjhMxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "def count_videos(dataset_path):\n",
        "    \"\"\"Count the number of videos in each class of the dataset and organize by class.\"\"\"\n",
        "    stats = {}\n",
        "    for split in ['train', 'val']:\n",
        "        split_path = os.path.join(dataset_path, split)\n",
        "        if os.path.exists(split_path):\n",
        "            for class_name in os.listdir(split_path):\n",
        "                class_path = os.path.join(split_path, class_name)\n",
        "                if os.path.isdir(class_path):\n",
        "                    videos = [f for f in os.listdir(class_path) if f.endswith(('.mp4', '.MOV', '.mov', '.avi'))]\n",
        "                    if class_name not in stats:\n",
        "                        stats[class_name] = {'train': 0, 'val': 0}\n",
        "                    stats[class_name][split] += len(videos)\n",
        "    return stats\n",
        "\n",
        "def save_stats_to_csv(stats, output_csv):\n",
        "    \"\"\"Save the dataset statistics to a CSV file with a structure that includes serial numbers and a total column.\"\"\"\n",
        "    with open(output_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['#', 'Class', 'Train', 'Val', 'Total'])\n",
        "        serial_number = 1\n",
        "        for class_name, splits in sorted(stats.items()):\n",
        "            total_videos = splits['train'] + splits['val']\n",
        "            writer.writerow([serial_number, class_name, splits['train'], splits['val'], total_videos])\n",
        "            serial_number += 1\n",
        "\n",
        "dataset_path = '/content/output2'\n",
        "output_csv = 'dataset_statistics.csv'\n",
        "\n",
        "# Count videos and save the statistics\n",
        "stats = count_videos(dataset_path)\n",
        "save_stats_to_csv(stats, output_csv)\n",
        "\n",
        "print(f'Statistics saved to {output_csv}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8NIUgGqs0QJ",
        "outputId": "50334880-5929-4e97-ed74-a28eca18f3d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics saved to dataset_statistics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chuyển video test (nếu có) vào val"
      ],
      "metadata": {
        "id": "ov-Z8evbmmJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the base paths for the test and val directories\n",
        "base_test_path = '/content/output2/test'\n",
        "base_val_path = '/content/output2/val'\n",
        "\n",
        "# Get a list of class folders in the test directory\n",
        "class_folders = os.listdir(base_test_path)\n",
        "\n",
        "# Iterate over each class folder\n",
        "for class_folder in class_folders:\n",
        "    test_class_path = os.path.join(base_test_path, class_folder)\n",
        "    val_class_path = os.path.join(base_val_path, class_folder)\n",
        "\n",
        "    # Ensure the corresponding folder in val exists, if not, create it\n",
        "    if not os.path.exists(val_class_path):\n",
        "        os.makedirs(val_class_path)\n",
        "\n",
        "    # Move each video file from the test class folder to the val class folder\n",
        "    for file in os.listdir(test_class_path):\n",
        "        # Check if the file is a video based on its extension\n",
        "        if file.lower().endswith(('.mp4', '.mov', 'MOV')):\n",
        "            # Define the source and destination paths for the file\n",
        "            src_file_path = os.path.join(test_class_path, file)\n",
        "            dest_file_path = os.path.join(val_class_path, file)\n",
        "\n",
        "            # Move the file\n",
        "            shutil.move(src_file_path, dest_file_path)\n",
        "            print(f\"Moved {file} from {test_class_path} to {val_class_path}\")\n",
        "\n",
        "print(\"Video files have been moved.\")\n"
      ],
      "metadata": {
        "id": "C0zuAoXXmrUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ6uAoaVjrOF",
        "outputId": "7bb1b91f-c1e3-4c0f-d7bd-e30929009c46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r '/content/drive/MyDrive/ai4life_2024/v2/v2_part1.zip' 'output2'"
      ],
      "metadata": {
        "id": "cR89IOHc82d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model"
      ],
      "metadata": {
        "id": "tTmaAo1UgOC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Địa chỉ của API Flask\n",
        "API_BASE_URL = 'http://103.188.243.119:5002'\n",
        "\n",
        "def upload_file(filepath):\n",
        "    \"\"\"Tải lên một file lên server.\"\"\"\n",
        "    url = f'{API_BASE_URL}/upload'\n",
        "    files = {'file': open(filepath, 'rb')}\n",
        "    response = requests.post(url, files=files)\n",
        "    return response.text\n",
        "\n",
        "def download_file(filename):\n",
        "    \"\"\"Tải xuống một file từ server.\"\"\"\n",
        "    url = f'{API_BASE_URL}/download/{filename}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"File '{filename}' downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to download file: {response.text}\")\n",
        "\n",
        "# Đường dẫn đến file bạn muốn tải lên\n",
        "# filepath_to_upload = '/content/yolov9/runs/train/exp/weights/best.pt'\n",
        "\n",
        "# Tải lên file\n",
        "# print(upload_file(filepath_to_upload))\n",
        "\n",
        "# Tải xuống file\n",
        "download_file('best_workout_3.pt')\n"
      ],
      "metadata": {
        "id": "QiLJPfr5iT8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a833cc6-95d5-401d-e9c6-9b41689b416a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'best_workout_3.pt' downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SkalskiP/yolov9.git\n",
        "%cd yolov9\n",
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "xKfHlEISjBHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1qo22TW0b83NpsXBHO4J12bGfPwDUpY5p"
      ],
      "metadata": {
        "id": "jwWQCx0FzNlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip v2_part1.zip"
      ],
      "metadata": {
        "id": "zwULiBGYzXZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XjFuvkVfgM79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/best.pt --conf 0.1 --source /content/anhtest.png --device 0"
      ],
      "metadata": {
        "id": "Su-Q58XyjkY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Function: inference image"
      ],
      "metadata": {
        "id": "VAqm1Vr0gTkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgr85tsefefT",
        "outputId": "3304fe13-e1f4-4a89-f2e6-39dd68d6295b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.0, 0.8926960825920105, (12.0, 97.0, 567.0, 362.0))]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.dataloaders import LoadImages\n",
        "from utils.general import non_max_suppression, scale_boxes\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "\n",
        "@smart_inference_mode()\n",
        "def run_object_detection(weights='/content/best_workout_2.pt',\n",
        "                         source='/content/anhtest.png',\n",
        "                         imgsz=(640, 640),\n",
        "                         conf_thres=0.25,\n",
        "                         iou_thres=0.45,\n",
        "                         device='0',\n",
        "                         classes=None,\n",
        "                         agnostic_nms=False):\n",
        "    # Setup\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weights, device=device, dnn=False)\n",
        "\n",
        "    # Dataloader\n",
        "    dataset = LoadImages(source, img_size=imgsz, stride=model.stride, auto=model.pt)\n",
        "\n",
        "    # List to store detections\n",
        "    detections = []\n",
        "\n",
        "    # Inference and processing\n",
        "    for path, img, im0s, vid_cap, s in dataset:\n",
        "        img = torch.from_numpy(img).to(device).float() / 255.0\n",
        "        if len(img.shape) == 3:\n",
        "            img = img[None]  # add batch dimension\n",
        "\n",
        "        pred = model(img, augment=False, visualize=False)\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n",
        "        pred_converted = []\n",
        "\n",
        "        for i, det in enumerate(pred):  # detections per image\n",
        "            if len(det):\n",
        "                det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0s.shape).round()\n",
        "\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    bbox = tuple(c.item() for c in xyxy)  # Convert bbox coordinates to tuple of floats\n",
        "                    pred_converted.append((cls.item(), conf.item(), bbox))  # Convert class and confidence to floats\n",
        "\n",
        "    return pred_converted\n",
        "\n",
        "# Example usage\n",
        "detections = run_object_detection()\n",
        "print(detections)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw bounding box"
      ],
      "metadata": {
        "id": "kRCm1pzqgZIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Load the original image\n",
        "image_path = '/content/anhtest.png'\n",
        "image = Image.open(image_path)\n",
        "# image = image.resize((640, 640))\n",
        "width_original, height_original = image.size\n",
        "\n",
        "# Calculate the resize scale factors\n",
        "scale_w = width_original / 640\n",
        "scale_h = height_original / 640\n",
        "\n",
        "detections_rescaled =detections\n",
        "#  [\n",
        "#     (cls, conf, [x_min * scale_w, y_min * scale_h, x_max * scale_w, y_max * scale_h])\n",
        "#     for cls, conf, (x_min, y_min, x_max, y_max) in detections]\n",
        "\n",
        "# Create a drawing context\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "# Draw the rescaled bounding boxes on the original image\n",
        "for det in detections_rescaled:\n",
        "    cls, conf, bbox = det\n",
        "    x_min, y_min, x_max, y_max = bbox\n",
        "    draw.rectangle(((x_min, y_min), (x_max, y_max)), outline=\"red\", width=2)\n",
        "    label = f\"Class: {cls}, Conf: {conf:.2f}\"\n",
        "    draw.text((x_min, y_min), label, fill=\"red\")\n",
        "\n",
        "# Display or save the image\n",
        "# image.show()\n",
        "# Or save to file\n",
        "image.save('output_image_with_rescaled_boxes.png')\n"
      ],
      "metadata": {
        "id": "0_HBYQqelAlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Function: Inference Video"
      ],
      "metadata": {
        "id": "NeaOqWjjgc1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.general import non_max_suppression, scale_boxes\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "import cv2  # Import OpenCV\n",
        "import numpy as np  # Ensure numpy is imported\n",
        "\n",
        "# Correctly placed letterbox function\n",
        "def letterbox(img, new_shape=(640, 640), color=(0, 0, 0)):\n",
        "    \"\"\"Resize image to a 32-pixel-multiple rectangle\"\"\"\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # width, height deltas\n",
        "    dw, dh = np.mod(dw, 64), np.mod(dh, 64)  # divide padding into 2 sides\n",
        "    dw /= 2\n",
        "    dh /= 2\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = round(dh - 0.1), round(dh + 0.1)\n",
        "    left, right = round(dw - 0.1), round(dw + 0.1)\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img\n",
        "\n",
        "@smart_inference_mode()\n",
        "def run_object_detection_video(weights='/content/best_workout_3.pt',\n",
        "                               source='/content/data_mae/train/lateral raise/lateral_raise_6.MOV',\n",
        "                               imgsz=(640, 640),\n",
        "                               conf_thres=0.5,\n",
        "                               iou_thres=0.5,\n",
        "                               device='0',\n",
        "                               classes=None,\n",
        "                               agnostic_nms=False):\n",
        "    # Setup\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weights, device=device, dnn=False)\n",
        "\n",
        "    # Video capture\n",
        "    vid_cap = cv2.VideoCapture(source)\n",
        "    assert vid_cap.isOpened(), f'Failed to open video {source}'\n",
        "\n",
        "    # List to store detections for each frame, including the frame number\n",
        "    frame_detections = []\n",
        "\n",
        "    frame_number = 0  # Initialize frame counter\n",
        "\n",
        "    # Process each frame\n",
        "    while True:\n",
        "        ret, frame = vid_cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_number += 1  # Increment frame counter\n",
        "\n",
        "        # Convert BGR (OpenCV format) to RGB\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize and pad frame to model input size\n",
        "        img = letterbox(frame, new_shape=imgsz)\n",
        "        img = img.transpose((2, 0, 1))  # HWC to CHW\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        img = torch.from_numpy(img).to(device).float() / 255.0\n",
        "        img = img[None]  # add batch dimension\n",
        "\n",
        "        pred = model(img, augment=False, visualize=False)\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n",
        "\n",
        "        detections = []\n",
        "        for i, det in enumerate(pred):  # detections per frame\n",
        "            if len(det):\n",
        "                det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], frame.shape).round()\n",
        "\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    bbox = tuple(c.item() for c in xyxy)  # Convert bbox coordinates to tuple of floats\n",
        "                    # Now including frame_number in the detection information\n",
        "                    detections.append((cls.item(), conf.item(), bbox, frame_number))\n",
        "\n",
        "        if detections:  # If there are detections in the current frame\n",
        "            frame_detections.append(detections)\n",
        "\n",
        "    vid_cap.release()\n",
        "    return frame_detections\n",
        "# Ensure the main execution part of the script is correctly aligned and outside any function definition\n",
        "video_detections = run_object_detection_video()\n",
        "print(video_detections)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "vpgIxWdYMmxN",
        "outputId": "4dcd22a3-68a9-4807-85be-540a299a335e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Failed to open video /content/data_mae/train/lateral raise/lateral_raise_6.MOV",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5ce15efd564f>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mframe_detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Ensure the main execution part of the script is correctly aligned and outside any function definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mvideo_detections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_object_detection_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_detections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5ce15efd564f>\u001b[0m in \u001b[0;36mrun_object_detection_video\u001b[0;34m(weights, source, imgsz, conf_thres, iou_thres, device, classes, agnostic_nms)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Video capture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mvid_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mvid_cap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Failed to open video {source}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# List to store detections for each frame, including the frame number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Failed to open video /content/data_mae/train/lateral raise/lateral_raise_6.MOV"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(video_detections)"
      ],
      "metadata": {
        "id": "4Zyo6QjWukNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw bounding box for video"
      ],
      "metadata": {
        "id": "PStvYxDxgmcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def draw_boxes_and_save_video(input_video_path, output_video_path, predictions):\n",
        "    # Mở video gốc\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "    # Lấy thông số frame rate và định dạng size của video\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Định nghĩa codec và tạo đối tượng VideoWriter\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_id = 1\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Vẽ bounding box cho từng frame dựa trên kết quả dự đoán\n",
        "        for pred in predictions:\n",
        "            pred = pred[0]\n",
        "            # print(pred)\n",
        "            if pred[3] == frame_id:  # Kiểm tra frame_id của prediction\n",
        "                id_class, confidence, bbox, _ = pred\n",
        "                x_min, y_min, x_max, y_max = bbox\n",
        "                # Vẽ bounding box\n",
        "                cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)\n",
        "                # Vẽ text\n",
        "                cv2.putText(frame, f'ID: {id_class}, Conf: {confidence:.2f}', (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "\n",
        "        # Viết frame đã chỉnh sửa vào file output\n",
        "        out.write(frame)\n",
        "\n",
        "        frame_id += 1\n",
        "\n",
        "    # Giải phóng và đóng tất cả\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Đường dẫn đến video gốc và video output\n",
        "input_video_path = '/content/shoulder press_11.mp4'\n",
        "output_video_path = 'output.mp4'\n",
        "\n",
        "# Giả sử đây là kết quả dự đoán\n",
        "# predictions = run_object_detection_video()\n",
        "\n",
        "# Gọi hàm để vẽ bounding box và lưu video\n",
        "draw_boxes_and_save_video(input_video_path, output_video_path, video_detections)"
      ],
      "metadata": {
        "id": "CaQFHfgrYMKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Function: Create video after detect and resize 224"
      ],
      "metadata": {
        "id": "4THFx85QhRfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def crop_and_resize_frames(input_video_path, output_video_path, predictions):\n",
        "    # Mở video gốc\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "    # Lấy thông số frame rate và định dạng size của video\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Định nghĩa codec và tạo đối tượng VideoWriter\n",
        "    # Output size được set cố định là 224x224\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (224, 224))\n",
        "\n",
        "    frame_id = 1\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        largest_bbox = None\n",
        "        max_area = 0\n",
        "        for pred in predictions:\n",
        "            if pred[0][3] == frame_id:  # Kiểm tra frame_id của prediction\n",
        "                _, _, bbox, _ = pred[0]\n",
        "                x_min, y_min, x_max, y_max = bbox\n",
        "                area = (x_max - x_min) * (y_max - y_min)\n",
        "                if area > max_area:\n",
        "                    max_area = area\n",
        "                    largest_bbox = bbox\n",
        "        if largest_bbox is not None:\n",
        "            x_min, y_min, x_max, y_max = largest_bbox\n",
        "            cropped_frame = frame[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
        "        else:\n",
        "            # Nếu không có bbox, lấy toàn bộ khung hình\n",
        "            cropped_frame = frame\n",
        "\n",
        "        # Resize khung hình về 224x224\n",
        "        resized_frame = cv2.resize(cropped_frame, (224, 224))\n",
        "\n",
        "        # Viết frame đã chỉnh sửa vào file output\n",
        "        out.write(resized_frame)\n",
        "\n",
        "        frame_id += 1\n",
        "\n",
        "    # Giải phóng và đóng tất cả\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "\n",
        "# input_video_path = '/content/data_mae/train/lateral raise/lateral_raise_6.MOV'\n",
        "# output_video_path = 'output.mp4'\n",
        "\n",
        "# crop_and_resize_frames(input_video_path, output_video_path, video_detections)"
      ],
      "metadata": {
        "id": "FMzQnZXfolPH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteritor all video"
      ],
      "metadata": {
        "id": "angwk9Flha3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_dir = \"/content/output2/train\"\n",
        "output_dir = \"/content/output3/train\"\n",
        "\n",
        "for className in os.listdir(data_dir):\n",
        "        class_dir = os.path.join(data_dir, className)\n",
        "        if os.path.isdir(class_dir):\n",
        "            for video_file in os.listdir(class_dir):\n",
        "                input_video_path = os.path.join(class_dir, video_file)\n",
        "                output_video_path = f\"{output_dir}/{className}/{video_file}\"\n",
        "                os.makedirs(f\"{output_dir}/{className}\", exist_ok=True)\n",
        "                print(output_video_path)\n",
        "                video_detections = run_object_detection_video(source = input_video_path)\n",
        "                crop_and_resize_frames(input_video_path, output_video_path, video_detections)"
      ],
      "metadata": {
        "id": "YWeDSotj1FS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcb1559-8e43-4dfe-a056-aaf4d8523f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_102.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_10.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_18.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_2.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_24.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_22.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_15.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_20.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_7.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_13.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_4.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_23.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_9.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_14.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_26.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_1.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_201.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_21.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_25.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_16.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_8.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_3.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_12.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_19.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_6.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_17.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_5.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_11.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/incline bench press/incline_bench_press_101.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_3.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_15.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_110.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_101.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_7.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_28.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_6.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_38.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_12.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_17.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_105.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_21.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_29.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_37.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_13.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_39.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_1.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_10.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_11.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_104.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_4.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_103.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_40.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_20.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_27.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_26.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_18.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_2.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_9.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_102.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_32.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_25.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_5.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_35.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_19.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_24.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_22.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
            "YOLOv5 🚀 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output3/train/tricep pushdown/tricep_pushdown_8.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r '/content/drive/MyDrive/ai4life_2024/v2/v2_part2.zip' 'output3'"
      ],
      "metadata": {
        "id": "vYznTlkN1gUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}